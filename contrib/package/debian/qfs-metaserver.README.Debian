Setting Up QFS
==============

Main components of the QFS server are the 'metaserver' and the 'chunkserver'.
Metaserver provides the namespace for the filesystem while the chunkservers do
the storage/retrieval of file blocks in the form of 'chunks'.

Each server uses a configuration file that sets the run time parameters of the
server. The metaserver is configured with the filesystem port, chunkserver
port, chunk placement groups for replication, the location of transaction
logs and checkpoints and so on. The chunk server is configured with the port
of the metaserver, path to copy the chunks and so on.

An easy set up of QFS has been provided in the examples/ directory, where a
metaserver and two chunk servers are launched, all on the same node. To do this
setup,

        $ cd ~/code/qfs
        $ make
        $ examples/sampleservers/sample_setup.py -a install

The python script creates config files for the QFS servers that can be found
under `~/qfsbase/` directory.

In this example setup, the metaserver listens on the filesystem port 20000 and
the webserver for monitoring filesystem listens on port 22000. Note that in
practice one chunkserver is deployed per QFS instance per host, but for the
purpose of illustration, this example setup uses two chunkservers per host.
Hence the capacity statistics are counted twice and reported as such via the
web UI at `http://localhost:22000`

For a more practical setup of QFS, please refer to the QFS Wiki documents at
https://github.com/quantcast/qfs/wiki/Deployment-Guide

Using QFS
=========

Once the QFS servers are up and running, one can use the QFS by different
means.

* Use client tools that are built during the compile process. For instance,

        $ cd ~/code/qfs
        $ PATH=${PWD}/build/release/bin/tools:${PATH}
        $ qfsshell -s localhost -p 20000 -q -- mkdir /tmp
        $ echo 'Hello World' | cptoqfs -s localhost -p 20000 -k /tmp/HW.dat -d -
        $ qfscat -s localhost -p 20000 /tmp/HW.dat
        $ qfsshell -s localhost -p 20000 -q -- rm /tmp/HW.dat

* If you built the QFS FUSE client, then you can mount the QFS at a local mount
  point by,

        $ mkdir /mnt/qfs
        $ cd ~/code/qfs/build/release/bin/
        $ ./qfs_fuse localhost:20000 /mnt/qfs -o allow_other,ro

  Further information about compiling and using QFS FUSE is at
  https://github.com/quantcast/qfs/wiki/Developer-Documentation

* Build your own QFS client in C++, Java, or Python (experimental) using the
  QFS client libraries that are generated during the compile process. See
  examples in directory `~/code/qfs/examples/` for reference.


FUSE
====
You can use the qfs_fuse binary directly or via /etc/fstab.

    Direct usage:
        Mount using $ sudo ./qfs_fuse <metaserver>:20000 /mnt/qfs -o allow_other,ro
        Unmount using $ sudo umount /mnt/qfs
    Editing /etc/fstab to mount automatically at startup:
        Create a symlink to qfs_fuse $ ln -s <path-to-qfs_fuse> /sbin/mount.qfs
        Add the following line to /etc/fstab:<metaserver>:20000 /mnt/qfs qfs ro,allow_other 0 0



Checkpoint and Transaction Log Pruning
======================================

The directories that store metaserver checkpoints (metaServer.cpDir) and
transaction logs (metaServer.logDir) must be pruned regularly; otherwise
they will fill up and run out of space. The pruning interval should be
based on the configured checkpoint frequency
(metaServer.checkpoint.interval).

To prune the checkpoints directory:

    qfs_checkpoint_prune.py /path/to/metaServer.cpDir

and for the transaction logs do:

    qfs_log_prune.py /path/to/metaServer.logDir



File System Backups
===================

Creating
--------


From time to time the metaServer.cpDir and metaServer.logDir should be
backed up, as they can be used to restore a file system which has had a
catastrophic failure. A backup consists of:

    The latest checkpoint file in metaServer.cpDir
    ALL of the transaction logs in metaServer.logDir

The simplest way to back up a file system image is to use tar to archive these files.

Given the following configuration:

metaServer.cpDir = /home/qfs0/state/checkpoint
metaServer.logDir = /home/qfs0/state/transactions

A possible solution would be to periodically do the following:

    tar --exclude '*.tmp.??????' -czf /foo/bar/qfs0-backup-`date +%d-%H.tar.gz` checkpoint transactions -C /home/qfs0/state

Note: this simple script includes all checkpoint files, which is
inefficient; only the latest checkpoint file is required for the backup.

Using date +%d-%H in the file name has the advantage of automatic backup
rotation, as it will roll over once a month. One backup file will be kept
for each hour of the day, every day of the month (e.g. 24 files for
September 26th, 24 files for September 27th, etc.). Backups should be
archived to a safe place, that is, not their own QFS file system. A
different QFS file system designated for backups with a high replication
count might be advisable.


Restoring
---------

To restore a backup, it need only be extracted to the appropriate
metaServer.cpDir and metaServer.logDir directories of a fresh metaserver
head node.

Using the configuration from the previous example:

    cd /home/qfs0/state && tar -xzf /foo/bar/qfs0-backup-31-23.tar.gz

Once the metaserver is started, it will read the latest checkpoint into
memory and replay any transaction logs. Files that were allocated since the
backup will no longer exist and chunks associated with these files will be
deleted. Files that have been deleted since the backup, however, will show
up as lost (as their chunks will have been deleted) but the restored file
system image will still reference them. After a restore, you should run a
file system integrity check and then delete the lost files it identifies.
Lastly, any other file modifications since the backup will be lost.

Note: The location of the metaServer.cpDir and metaServer.logDir should not change.


File System Integrity (qfsfsck)
===============================

The qfsfsck tool can be employed in three ways:

 * Verify the integrity of a running file system by identifying lost files
   and/or files with chunk placement/replication problems.
 * Validate the active checkpoint and transaction logs of a running file
   system.
 * Check the integrity of a file system archive/backup (checkpoint plus a
   set of transaction logs).

In order to verify the integrity of a running file system by identifying
lost files or files with chunk placement problems, run:

    qfsfsck -m metaServer.hostname -p metaServer.port


### Web UI

    http://localhost:20050/

####
## https://github.com/quantcast/qfs/wiki/Deployment-Guide

NOTE: This configuration does not preclude the use of chunk replication, as
the client sets the fault tolerance of each file individually when it is
created.

Reed-Solomon <rs k,m+n>
k 	replication factor, normally 1 with n == 3, and more than 1 with n == 0
m 	data stripe count, valid range is 1 to 64 with n == 3, and 255 with n == 0
n 	recovery stripe count, only 0 and 3 are presently valid, with n == 0 pure striping -- no recovery

Currently the only officially supported encodings are <rs 1,6+3> and
replication 3 (expressed as <rs 3,6+0). Theoretically, the data stripe
count could be increased to improve recovery time by spreading recovery
data across a greater number of nodes (thus adding network bandwidth);
however, this configuration is not tested or supported.


-----

Create a file containing "Hello World", Reed-Solomon encoded, replication 1.

    echo 'Hello World' | cptoqfs -s localhost -p 20000 -S -r 1 -k /qfs/tmp/helloworld -d -


Stat the file to see encoding (RS or not), replication level, and mtime.

    qfsshell -s localhost -p 20000 -q -- stat /qfs/tmp/helloworld
